{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2_PartA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WuiCFS-bK1e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yingReD0QyhK"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S9qlmY7Tbpm"
      },
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUxYr82I0HsZ"
      },
      "source": [
        "!unzip '/content/drive/MyDrive/nature_12K'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mANj6GJvw6ec"
      },
      "source": [
        "#Question 1 (5 Marks)\n",
        "\n",
        "\n",
        "\n",
        "Build a small CNN model consisting of  5 convolution layers. Each convolution layer would be followed by a ReLU activation and a max pooling layer. Here is sample code for building one such conv-relu-maxpool block in keras. \n",
        "\n",
        "After 5 such conv-relu-maxpool blocks of  layers you should have one dense layer followed by the output layer containing 10 neurons (1 for each of the 10 classes). The input layer should be compatible with the images in the iNaturalist dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hwbMhDEFrBf"
      },
      "source": [
        "def model_framework(): \n",
        "#---------DEFAULT HYPERPARAMETERS-----------------------------------------------\n",
        "  hyperparameter_defaults = dict(\n",
        "        max_epoch = 10,\n",
        "        num_filters = 128,\n",
        "        filter_org = 'double',\n",
        "        data_augmentation = 'N',\n",
        "        dropout = 0.4,\n",
        "        batch_normalization = 'Y',\n",
        "        optimizer = 'nag'\n",
        "      )\n",
        "  \n",
        "#----------------SETTING UP WANDB-----------------------------------------------\n",
        "  wandb.init(project=\"Assignment 2\", config=hyperparameter_defaults)\n",
        "  config = wandb.config\n",
        "  wandb.run.name = \"{}_epoch_{}_filt_{}_FiltOrg_{}_DataAug_{}_dropout_{}_bn_{}_opt\".format(config.max_epoch, config.num_filters,config.filter_org, config.data_augmentation, config.dropout, config.batch_normalization, config.optimizer)\n",
        "\n",
        "#-------------------OPTIMIZERS--------------------------------------------------\n",
        "  learning_rate = 1e-3\n",
        "  decay_rate = learning_rate / config.max_epoch\n",
        "  momentum = 0.8\n",
        "  if config.optimizer == 'adam':\n",
        "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate, decay=decay_rate)\n",
        "  elif config.optimizer == 'nadam':\n",
        "    optimizer = tf.keras.optimizers.Nadam(lr=learning_rate, decay=decay_rate)\n",
        "  elif config.optimizer == 'rmsprop':\n",
        "    optimizer = tf.keras.optimizers.RMSprop(lr=learning_rate, decay=decay_rate)\n",
        "  elif config.optimizer == 'nag':\n",
        "    optimizer = tf.keras.optimizers.SGD(lr=learning_rate, decay=decay_rate, nesterov=True)\n",
        "\n",
        "#-------------------DATA AUGMENTATION-------------------------------------------\n",
        "  IMG_SIZE = 256\n",
        "\n",
        "  data_aug = tf.keras.Sequential([preprocessing.RandomFlip(),\n",
        "                                  preprocessing.RandomCrop(IMG_SIZE, IMG_SIZE),\n",
        "                                  preprocessing.RandomRotation(factor = (-0.2, 0.2)),\n",
        "                                  preprocessing.RandomTranslation(height_factor=(-0.2, 0.2), \n",
        "                                                                  width_factor=(-0.2,0.2)),\n",
        "                                ])\n",
        "\n",
        "#-------------------MODEL FRAMEWORK---------------------------------------------\n",
        "  nf = config.num_filters\n",
        "\n",
        "  if config.data_augmentation == \"Y\":\n",
        "    model = Sequential([data_aug])\n",
        "  elif config.data_augmentation == \"N\":\n",
        "    model = Sequential()\n",
        "\n",
        "  model.add(Conv2D(nf, (3, 3), input_shape=(256, 256, 3),name = 'conv1'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2),strides = 1))\n",
        "\n",
        "  for i in range(1,5):\n",
        "    if config.filter_org == 'double':\n",
        "      nf = nf*2\n",
        "    elif config.filter_org == 'half':\n",
        "      nf = nf/2\n",
        "    model.add(Conv2D(nf, (3, 3),name = 'conv{}'.format(i+1)))\n",
        "    model.add(Activation('relu'))\n",
        "    if config.batch_normalization == 'Y':\n",
        "      model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(64))\n",
        "  model.add(Dropout(config.dropout))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(10))\n",
        "  model.add(Activation('softmax'))\n",
        " \n",
        " #------------MODEL COMPILATION--------------------------------------\n",
        "  model.compile(optimizer=optimizer,\n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "  metrics=['accuracy'])\n",
        "\n",
        "  return model,config \n",
        "\n",
        "def train(model,config):\n",
        "#------------MODEL TRAINING--------------------------------------\n",
        "\n",
        "  model.fit(train_dataset, validation_data = validation_dataset, batch_size = 32, epochs = config.max_epoch, callbacks=[WandbCallback()] )\n",
        "  \n",
        "  model.summary()   \n",
        "\n",
        "  return model\n",
        "\n",
        "def test(model):\n",
        "#-----------MODEL EVALUATION WITH TEST DATA-------------------------------------\n",
        "  wandb.init(project=\"Assignment 2\")\n",
        "  wandb.run.name = \"Test_run_for_best_model\"\n",
        "  results = model.evaluate(test_dataset, batch_size=128,callbacks=[WandbCallback()])\n",
        "  wandb.log({'test_loss': results[0], 'test_accuracy': results[1]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52WKCnXgubgl"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf \n",
        "from tensorflow.keras.models import Sequential,load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras.applications.resnet import ResNet50, preprocess_input, decode_predictions\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras import backend as K\n",
        "from wandb.keras import WandbCallback\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.random import seed\n",
        "from numpy.random import randint\n",
        "import pandas as pd\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDEkeVPTOk9c"
      },
      "source": [
        "#Question 2 (10 Marks)\n",
        "\n",
        "\n",
        "\n",
        "You will now train your model using the iNaturalist dataset. The zip file contains a train and a test folder. Set aside 10% of the training data for hyperparameter tuning. Make sure each class is equally represented in the validation data. Do not use the test data for hyperparameter tuning. Using the sweep feature in wandb find the best hyperparameter configuration. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdA3NoTsuWX8",
        "outputId": "d31a06c9-b45f-4abc-a5e5-6caf16d6b144"
      },
      "source": [
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory('/content/inaturalist_12K/train',labels = 'inferred', \n",
        "                                                                    validation_split=0.1, seed = 123, subset = \"training\")\n",
        "\n",
        "validation_dataset = tf.keras.preprocessing.image_dataset_from_directory('/content/inaturalist_12K/train', labels = 'inferred',\n",
        "                                                                    validation_split=0.1, seed = 123, subset = \"validation\")\n",
        "\n",
        "test_dataset = tf.keras.preprocessing.image_dataset_from_directory('/content/inaturalist_12K/val', labels = 'inferred',seed = 123)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9999 files belonging to 10 classes.\n",
            "Using 9000 files for training.\n",
            "Found 9999 files belonging to 10 classes.\n",
            "Using 999 files for validation.\n",
            "Found 2000 files belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUvyWQK-1srI"
      },
      "source": [
        "#--------------------------SWEEP HYPERPARAMETERS--------------------------------\n",
        "sweep_config = {\n",
        "  \"name\": \"My Sweep\",\n",
        "  \"method\": \"random\",\n",
        "  \"project\": \"Assignment02\",\n",
        "  \"metric\":{\n",
        "      \"name\":\"val_accuracy\",\n",
        "      \"goal\":\"maximize\"\n",
        "  },\n",
        "  \"parameters\": {\n",
        "        \"max_epoch\": {\n",
        "            \"values\": [10,20]\n",
        "        },\n",
        "        \"num_filters\": {\n",
        "            \"values\":[16,32,64]\n",
        "        }, \n",
        "        \"filter_org\": {\n",
        "            \"values\":['same','double','half']\n",
        "        },\n",
        "        \"data_augmentation\":{\n",
        "            \"values\":['Y',\"N\"]\n",
        "        },\n",
        "        \"dropout\":{\n",
        "            \"values\":[0, 0.2,0.3,0.4]\n",
        "        },\n",
        "        \"batch_normalization\": {\n",
        "            \"values\":['Y',\"N\"]\n",
        "        },  \n",
        "        \"optimizer\": {\n",
        "            \"values\":['adam', 'nadam','rmsprop','nag']\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut64UvbB5z1i"
      },
      "source": [
        "#-------------------------CODE FOR ACTUAL SWEEP---------------------------------\n",
        "model,config = model_framework()\n",
        "wandb.agent(sweep_id, function=train(model,config))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qnNb0aVO58K"
      },
      "source": [
        "#Question 4 (5 Marks)\n",
        "\n",
        "\n",
        "\n",
        "You will now apply your best model on the test data (You shouldn't have used test data so far. All the above experiments should have been done using train and val data only). \n",
        "\n",
        "\n",
        "\n",
        "(a) Use the best model from your sweep and report the accuracy on the test set. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7J12WHIMYWL"
      },
      "source": [
        "#-----------------LOADING THE BEST MODEL FROM THE SWEEP-------------------------\n",
        "model = tf.keras.models.load_model(wandb.restore('model-best.h5', run_path=\"kodikarthik21/Assignment 2/m5pfdn04\").name)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cszcnMXyr5G3"
      },
      "source": [
        "#-------------FINDING TEST LOSS AND ACCURACY FOR THE BEST MODEL-----------------\n",
        "test(model)\n",
        "wandb.run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMFZqT_HPE1g"
      },
      "source": [
        "(b) Provide a 10 x 3 grid containing sample images from the test data and predictions made by your best model (more marks for presenting this grid creatively)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FEG2K-QszJi"
      },
      "source": [
        "#-----------COMPARING PREDICTED AND ACTUAL OUTPUTS FROM TEST DATASET------------\n",
        "\n",
        "class_names = test_dataset.class_names\n",
        "#model = tf.keras.models.load_model(wandb.restore('model-best.h5', run_path=\"kodikarthik21/Assignment 2/m5pfdn04\").name)\n",
        "ds = test_dataset.take(1)\n",
        "y_prob = model.predict(ds) \n",
        "y_classes = y_prob.argmax(axis=-1)\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images,labels in ds:\n",
        "  for i in range(30):\n",
        "    wandb.init(project=\"Assignment 2\")\n",
        "    wandb.run.name = \"Actual: {}\".format(class_names[labels[i]])  \n",
        "    #ax = plt.subplot(4,3,(j+1,i+1))\n",
        "    #plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    if labels[i] == y_classes[i]:\n",
        "       wandb.log({\"Test\": [wandb.Image(images[i].numpy().astype(\"uint8\"), caption=\"PREDICTED SAME AS ACTUAL\")]})\n",
        "    else:\n",
        "      wandb.log({\"Test\": [wandb.Image(images[i].numpy().astype(\"uint8\"), caption=\"Predicted: {}\".format(class_names[y_classes[i]]))]})\n",
        "    #plt.title([class_names[labels[k]],class_names[y_classes[i]]])\n",
        "    #plt.axis(\"off\")\n",
        "    wandb.run.finish()\n",
        "  break;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnUkK_mNPJhA"
      },
      "source": [
        "(c) Visualise all the filters in the first layer of your best model for a random image from the test set. If there are 64 filters in the first layer plot them in an 8 x 8 grid. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MulGr7A9Lk1x"
      },
      "source": [
        "#---------------VISUALIZING FILTERS IN THE 1ST CONV LAYER-----------------------\n",
        "\n",
        "# model = tf.keras.models.load_model(wandb.restore('model-best.h5', run_path=\"kodikarthik21/Assignment 2/m5pfdn04\").name)\n",
        "model = Model(inputs=model.inputs, outputs=model.layers[0].output)\n",
        "\n",
        "img = load_img(\"inaturalist_12K/val/Mammalia/9e014930ee1bd2f4079320ec01ce8858.jpg\")\n",
        "img = img_to_array(img)\n",
        "img = np.expand_dims(img, axis=0)\n",
        "img = load_image(\"inaturalist_12K/val/Mammalia/9e014930ee1bd2f4079320ec01ce8858.jpg\")\n",
        "feature_maps = model.predict(img)\n",
        "for i in range(128):\n",
        "  wandb.init(project=\"Assignment 2\")\n",
        "  wandb.run.name = \"Filter_viz {}\".format(i+1)  \n",
        "  wandb.log({\"Filter\": [wandb.Image(feature_maps[0, :, :, i], caption=i+1)]})\n",
        "  wandb.run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6gl6xSuPMtH"
      },
      "source": [
        "#Question 5 (10 Marks)\n",
        "\n",
        "\n",
        "\n",
        "Apply guided back propagation on any 10 neurons in the CONV5 layer and plot the images which excite this neuron. The idea again is to discover interesting patterns which excite some neurons. You will draw a 10 x 1 grid below with one image for each of the 10 neurons.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i_ltOFWW5Sm"
      },
      "source": [
        "#-----------GUIDED BACKPROPAGATION PRE AND POST PROCESS FUNCTIONS---------------\n",
        "H, W = 256, 256\n",
        "\n",
        "#---------Processing images------------\n",
        "def load_image(path, preprocess=True):\n",
        "    \"\"\"Load and preprocess image.\"\"\"\n",
        "    x = image.load_img(path, target_size=(H, W))\n",
        "    if preprocess:\n",
        "        x = image.img_to_array(x)\n",
        "        x = np.expand_dims(x, axis=0)\n",
        "        x = preprocess_input(x)\n",
        "    return x\n",
        "\n",
        "#---------Deprocessing images---------\n",
        "def deprocess_image(x):\n",
        "    #-------Normalizing the images--------\n",
        "    x = x.copy()\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + K.epsilon())\n",
        "    x *= 0.25\n",
        "\n",
        "    #------Make output between 0 and 1----\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    #------Coloring-------\n",
        "    x *= 255\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK4v_AcRURji"
      },
      "source": [
        "#------------GUIDED BACKPROPAGATION---------------------------------------------\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 12))\n",
        "\n",
        "wandb.init(project=\"Assignment 2\")\n",
        "wandb.run.name = \"Guided BackPropagation\"\n",
        "\n",
        "#-----------Load an image-------------\n",
        "plt.imshow(load_image(\"inaturalist_12K/val/Mammalia/9e014930ee1bd2f4079320ec01ce8858.jpg\", preprocess=False))\n",
        "plt.axis(\"off\");\n",
        "plt.figure(figsize=(60, 60))\n",
        "wandb.log({\"Guided_BackProp\": [wandb.Image(load_image(\"inaturalist_12K/val/Mammalia/9e014930ee1bd2f4079320ec01ce8858.jpg\", preprocess=False), caption='Original')]})\n",
        "plt.axis(\"off\");\n",
        "preprocessed_input = load_image(\"inaturalist_12K/val/Mammalia/9e014930ee1bd2f4079320ec01ce8858.jpg\")\n",
        "\n",
        "#-----------Guided Relu to remove negative gradients-------------\n",
        "ax = plt.subplot(1,10,i+1)\n",
        "@tf.custom_gradient\n",
        "def guidedRelu(x):\n",
        "  def grad(dy):\n",
        "    return tf.cast(dy>0,\"float32\") * tf.cast(x>0, \"float32\") * dy\n",
        "  return tf.nn.relu(x), grad\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "  \n",
        "  wandb.init(project=\"Assignment 2\")\n",
        "  wandb.run.name = \"Guided BackPropagation {}\".format(i+1)  \n",
        "\n",
        "  # model = tf.keras.models.load_model(wandb.restore('model-best.h5', run_path=\"kodikarthik21/uncategorized/m5pfdn04\").name) \n",
        "  # model.summary()\n",
        "\n",
        "  #-----------Get outputs from CONV5 layer------------\n",
        "  gb_model = Model(\n",
        "      inputs = [model.inputs],\n",
        "      outputs = [model.get_layer('conv2d_4').output]  # Numbering starts from 0. Therefore 5th conv layer is conv2d_4\n",
        "  )\n",
        "\n",
        "  #-----------Selecting neurons at random from CONV5 Layer and doing Guided Backprop\n",
        "  seed(i+6)\n",
        "  rand2 = randint(outputs.shape[1])\n",
        "  rand3 = randint(outputs.shape[2])\n",
        "  rand4 = randint(outputs.shape[3])\n",
        "\n",
        "  #-----------Creating a dummy array------------\n",
        "  modif = np.zeros(outputs.shape)\n",
        "  modif[0][rand2][rand3][rand4] = 1; # Array to be multiplied with output for guided backprop\n",
        "\n",
        "  #-----------Changing activation to Guided Relu to make negative gradients to ZERO-----------\n",
        "  layer_dict = [layer for layer in gb_model.layers[1:] if hasattr(layer,'activation')]\n",
        "  for layer in layer_dict:\n",
        "    if layer.activation == tf.keras.activations.relu:\n",
        "      layer.activation = guidedRelu\n",
        "\n",
        "  #-----------Making all the other inputs ZERO------------\n",
        "  with tf.GradientTape() as tape:\n",
        "    inputs = tf.cast(preprocessed_input, tf.float32)\n",
        "    tape.watch(inputs)\n",
        "    outputs = gb_model(inputs)*modif # To assign all outputs to zero and only this to one\n",
        "\n",
        "  #-----------Calculate gradients with the updated outputs and remove negative gradients------------\n",
        "  grads = tape.gradient(outputs,inputs)[0]\n",
        "  plt.subplot(1,10,i+1)\n",
        "  plt.imshow(np.flip(deprocess_image(np.array(grads)),-1))\n",
        "  wandb.log({\"Guided_BackProp\": [wandb.Image(np.flip(deprocess_image(np.array(grads)),-1), caption=i+1)]})\n",
        "  plt.axis(\"off\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za4VgKPMWq_v"
      },
      "source": [
        ""
      ]
    }
  ]
}